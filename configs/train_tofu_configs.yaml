
num_proc: 16
seed: 0

exp_name: "tofu_5epoch_masked"
out_directory: "/home/ryan/decouple_share/models/${exp_name}"

data_to_load: "tofu"

model_path_or_name: "allenai/OLMo-1B-hf"
tokenizer_path_or_name: "${model_path_or_name}"

wandb:
  do: False
  project: "decouple"
  group: "OLMO-1B_tofu"
  name: "5epoch_masked"

max_seq_len: 256

save_model: True

apply_loss_mask: True # this is only used for tofu training

training_args:
  per_device_train_batch_size: 16
  gradient_accumulation_steps: 2
  num_train_epochs: 5
  fp16: True
  learning_rate: 1e-5
  warmup_ratio: 0.03
  logging_steps: 2

# note: this only applies to training that uses in-training evaluation (tofu does not)
eval:
  eval_steps: 10
  per_device_eval_batch_size: 16

lora:
  do: False
  lora_modules: [ "q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj", "lm_head"] # for olmo and llama

